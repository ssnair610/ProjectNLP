{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1ff056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch import Tensor\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b4a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = os.path.join(\"Data\", \"track-a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504f5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34521e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpModel = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bc303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerFunction(text):\n",
    "    tempDoc = nlpModel(text)\n",
    "    token = [\n",
    "        tok.lemma_.lower()\n",
    "        for tok in tempDoc\n",
    "        if not tok.is_stop and not tok.is_punct and tok.lemma_ != \"-PRON-\"\n",
    "    ]\n",
    "    return \" \".join(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db73cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame[\"Spacy_text\"] = dataFrame[\"text\"].astype(str).apply(cleanerFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d34042",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColumns = [col for col in [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"] if col in dataFrame]\n",
    "yData = dataFrame[labelColumns].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712fc7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xAll = dataFrame[\"Spacy_text\"].tolist()\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xAll, yData, test_size = 0.1, random_state = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a3dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = xTrain.copy()\n",
    "xVal = xVal.copy()\n",
    "\n",
    "numLabels = yTrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d276d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "userDevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a548964",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_PATTERN = re.compile(r\"\\b\\w+\\b\")\n",
    "\n",
    "def simpleTokenize(text: str) -> list:\n",
    "    return TOKEN_PATTERN.findall(text.lower())\n",
    "\n",
    "\n",
    "def vocabularyBuilder(texts: list, vocabsize: int):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        tokens = simpleTokenize(t)\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    mostCommon = counter.most_common(vocabsize - 2)\n",
    "    indextoword = [\"<pad>\", \"<unk>\"] + [token for token, _ in mostCommon]\n",
    "    wordtoindex = {w: i for i, w in enumerate(indextoword)}\n",
    "\n",
    "    return wordtoindex, indextoword\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "wordtoindex, indextoword = vocabularyBuilder(xTrain, vocabsize = MAX_VOCAB_SIZE)\n",
    "vocabSize = len(indextoword)\n",
    "\n",
    "def encodePadFunction(texts: list, wordtoindex: dict, sequenceLength: int = MAX_SEQUENCE_LENGTH) -> np.ndarray:\n",
    "    encodings = []\n",
    "    padIndex = wordtoindex[\"<pad>\"]\n",
    "    unkIndex = wordtoindex[\"<unk>\"]\n",
    "\n",
    "    for t in texts:\n",
    "        tokens = simpleTokenize(t)\n",
    "        tokenIDs = [wordtoindex.get(tok, unkIndex) for tok in tokens]\n",
    "        if len(tokenIDs) > sequenceLength:\n",
    "            tokenIDs = tokenIDs[:sequenceLength]\n",
    "        else:\n",
    "            tokenIDs = tokenIDs + [padIndex] * (sequenceLength - len(tokenIDs))\n",
    "        encodings.append(tokenIDs)\n",
    "\n",
    "    return np.array(encodings, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f697cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainEncode = encodePadFunction(xTrain, wordtoindex, sequenceLength = MAX_SEQUENCE_LENGTH)\n",
    "valEncode = encodePadFunction(xVal, wordtoindex, sequenceLength = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6deb6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocabSize: int,\n",
    "            embeddingDim: int,\n",
    "            hiddenUnit1: int,\n",
    "            hiddenUnit2: int,\n",
    "            dropoutRate: float,\n",
    "            numLabels: int,\n",
    "            padIndex: int\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocabSize, embeddingDim, padding_idx = padIndex)\n",
    "        self.dropout1 = nn.Dropout(dropoutRate)\n",
    "        self.fc1 = nn.Linear(embeddingDim, hiddenUnit1)\n",
    "        self.bn1 = nn.BatchNorm1d(hiddenUnit1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropoutRate)\n",
    "        self.fc2 = nn.Linear(hiddenUnit1, hiddenUnit2)\n",
    "        self.bn2 = nn.BatchNorm1d(hiddenUnit2)\n",
    "        self.dropout3 = nn.Dropout(dropoutRate)\n",
    "        self.fc_out = nn.Linear(hiddenUnit2, numLabels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)             \n",
    "        avgEmb = emb.mean(dim = 1) \n",
    "        h1 = self.fc1(avgEmb)\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = self.relu(h1)\n",
    "        h1 = self.dropout2(h1)\n",
    "        h2 = self.fc2(h1)\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = self.relu(h2)\n",
    "        h2 = self.dropout3(h2)\n",
    "        return self.fc_out(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c28119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabSize: int,\n",
    "        embeddingDim: int,\n",
    "        hiddenDim: int,\n",
    "        rnnLayers: int,\n",
    "        bidirectional: bool,\n",
    "        dropoutRate: float,\n",
    "        denseUnits: int,\n",
    "        numLabels: int,\n",
    "        padIndex: int,\n",
    "        useAttention: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels = embeddingDim,\n",
    "            out_channels = embeddingDim,\n",
    "            kernel_size = 5,\n",
    "            padding = 2\n",
    "        )\n",
    "        self.relu_cnn = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size = 2)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabSize, embeddingDim, padding_idx = padIndex)\n",
    "        self.dropout_emb = nn.Dropout(dropoutRate)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embeddingDim,\n",
    "            hidden_size = hiddenDim,\n",
    "            num_layers = rnnLayers,\n",
    "            bidirectional = bidirectional,\n",
    "            batch_first = True,\n",
    "            dropout = dropoutRate if rnnLayers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.use_attention = useAttention\n",
    "        directionFactor = 2 if bidirectional else 1\n",
    "        if useAttention:\n",
    "            self.attn_linear = nn.Linear(hiddenDim * directionFactor, hiddenDim * directionFactor)\n",
    "            self.attn_v = nn.Linear(hiddenDim * directionFactor, 1, bias = False)\n",
    "\n",
    "        self.fc1 = nn.Linear(hiddenDim * directionFactor, denseUnits)\n",
    "        self.bn1 = nn.BatchNorm1d(denseUnits)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropoutFc = nn.Dropout(dropoutRate)\n",
    "        self.output_layer = nn.Linear(denseUnits, numLabels)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> Tensor:\n",
    "        pad_idx = self.embedding.padding_idx\n",
    "        lengths = (x != pad_idx).sum(dim = 1)\n",
    "        lengths = torch.clamp(lengths, min = 1)\n",
    "\n",
    "        embTensor: Tensor = self.embedding(x)  # type: ignore\n",
    "        embTensor = self.dropout_emb(embTensor)   #type: ignore\n",
    "\n",
    "        c_in = embTensor.transpose(1, 2)              \n",
    "        c_out = self.relu_cnn(self.conv1d(c_in))\n",
    "        c_out = self.pool(c_out)\n",
    "\n",
    "        rnn_in = c_out.transpose(1, 2)\n",
    "        lengths = torch.clamp(lengths // 2, min = 1)\n",
    "\n",
    "        packed = pack_padded_sequence(rnn_in, lengths.cpu(), batch_first = True, enforce_sorted = False)\n",
    "        rnnOut, _ = self.lstm(packed)\n",
    "        rnnOut, _ = pad_packed_sequence(rnnOut, batch_first = True)\n",
    "\n",
    "        if self.use_attention:\n",
    "            scores = torch.tanh(self.attn_linear(rnnOut))\n",
    "            weights = torch.softmax(self.attn_v(scores), dim = 1)\n",
    "            finalFeat = (weights * rnnOut).sum(dim = 1)\n",
    "        else:\n",
    "            idx = torch.arange(x.size(0), device = x.device)\n",
    "            finalFeat = rnnOut[idx, lengths - 1]\n",
    "\n",
    "        h: Tensor = self.fc1(finalFeat) #type: ignore\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropoutFc(h)  #type: ignore\n",
    "        logits: Tensor = self.output_layer(h)   #type: ignore\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bacd4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabSize: int,\n",
    "        embeddingDim: int,\n",
    "        hiddenDim: int,\n",
    "        rnnLayers: int,\n",
    "        bidirectional: bool,\n",
    "        dropoutRate: float,\n",
    "        denseUnits: int,\n",
    "        numLabels: int,\n",
    "        padIndex: int,\n",
    "        useAttention: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels = embeddingDim,\n",
    "            out_channels = embeddingDim,\n",
    "            kernel_size = 5,\n",
    "            padding = 2\n",
    "        )\n",
    "        self.relu_cnn = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size = 2)\n",
    "\n",
    "        self.embedding: nn.Embedding = nn.Embedding(vocabSize, embeddingDim, padding_idx = padIndex)\n",
    "        self.dropout_emb: nn.Dropout = nn.Dropout(dropoutRate)\n",
    "        self.gru: nn.GRU = nn.GRU(\n",
    "            input_size = embeddingDim,\n",
    "            hidden_size = hiddenDim,\n",
    "            num_layers = rnnLayers,\n",
    "            bidirectional = bidirectional,\n",
    "            batch_first = True,\n",
    "            dropout = dropoutRate if rnnLayers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.use_attention: bool = useAttention\n",
    "        factor = 2 if bidirectional else 1\n",
    "        if useAttention:\n",
    "            self.attn_linear: nn.Linear = nn.Linear(hiddenDim * factor, hiddenDim * factor)\n",
    "            self.attn_v: nn.Linear = nn.Linear(hiddenDim * factor, 1, bias = False)\n",
    "\n",
    "        self.fc1: nn.Linear = nn.Linear(hiddenDim * factor, denseUnits)\n",
    "        self.bn1: nn.BatchNorm1d = nn.BatchNorm1d(denseUnits)\n",
    "        self.relu: nn.ReLU = nn.ReLU()\n",
    "        self.dropout_fc: nn.Dropout = nn.Dropout(dropoutRate)\n",
    "        self.output_layer: nn.Linear = nn.Linear(denseUnits, numLabels)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> Tensor:\n",
    "        lengths = (x != self.embedding.padding_idx).sum(dim = 1)\n",
    "        lengths = torch.clamp(lengths, min = 1)\n",
    "\n",
    "        emb = self.dropout_emb(self.embedding(x))\n",
    "\n",
    "        c_in = emb.transpose(1, 2)\n",
    "        c_out = self.relu_cnn(self.conv1d(c_in))\n",
    "        c_out = self.pool(c_out)\n",
    "\n",
    "        rnn_in = c_out.transpose(1, 2)\n",
    "        lengths = torch.clamp(lengths // 2, min = 1)\n",
    "\n",
    "        packed = pack_padded_sequence(rnn_in, lengths.cpu(), batch_first = True, enforce_sorted = False)\n",
    "        rnnOut, _ = self.gru(packed)\n",
    "        rnnOut, _ = pad_packed_sequence(rnnOut, batch_first = True)\n",
    "\n",
    "        if self.use_attention:\n",
    "            scores = torch.tanh(self.attn_linear(rnnOut))\n",
    "            weights = torch.softmax(self.attn_v(scores), dim = 1)\n",
    "            finalFeat = (weights * rnnOut).sum(dim = 1)\n",
    "        else:\n",
    "            idx = torch.arange(x.size(0), device = x.device)\n",
    "            finalFeat = rnnOut[idx, lengths - 1]\n",
    "\n",
    "        h: Tensor = self.fc1(finalFeat)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.dropout_fc(h)\n",
    "        logits: Tensor = self.output_layer(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d69fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class metrics_rfc():\n",
    "\n",
    "    def __init__(self, confusion_mat, log_level, title) -> None:\n",
    "        self.confusion_mat = confusion_mat\n",
    "        self.log_level = log_level\n",
    "        self.title = title\n",
    "\n",
    "    def accuracy(self, tp, tn, fp, fn):\n",
    "        return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    def precision(self, tp, fp):\n",
    "        return tp / (tp + fp)\n",
    "\n",
    "    def recall(self, tp, fn):\n",
    "        return tp / (tp + fn)\n",
    "\n",
    "    def f1_score(self, tp,fp,fn):\n",
    "        precision_val = self.precision(tp,fp)\n",
    "        recall_val = self.recall(tp,fn)\n",
    "        return 2 * (precision_val * recall_val) / (precision_val + recall_val)\n",
    "\n",
    "    def print_eval(self, title, accuracy, precision, recall, f1_score):\n",
    "        print(f\"{title}\\n\")\n",
    "        print(f\"accuracy: {round(accuracy,2)}\")\n",
    "        print(f\"precision: {round(precision,2)}\")\n",
    "        print(f\"recall: {round(recall,2)}\")\n",
    "        print(f\"f1 Score: {round(f1_score,2)}\")\n",
    "        print(\"=======\\n\")\n",
    "\n",
    "\n",
    "    def present_data(self):\n",
    "        total_accuracy = 0\n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        total_f1_score = 0\n",
    "\n",
    "        for i in range(0, len(self.confusion_mat)):\n",
    "            tp, fp = self.confusion_mat[i][0]\n",
    "            fn, tn = self.confusion_mat[i][1]\n",
    "\n",
    "            accuracy_val = self.accuracy(tp, tn, fp, fn)\n",
    "            precision_val = self.precision(tp, fp)\n",
    "            recall_val = self.recall(tp, fn)\n",
    "            f1_score_val = self.f1_score(tp, fp, fn)\n",
    "\n",
    "            total_accuracy += accuracy_val\n",
    "            total_precision += precision_val\n",
    "            total_recall += recall_val\n",
    "            total_f1_score += f1_score_val\n",
    "\n",
    "            if(self.log_level == \"emotions\"):\n",
    "                self.print_eval(\n",
    "                f\"Emotion: {labelColumns[i]}\",\n",
    "                    accuracy_val,\n",
    "                    precision_val,\n",
    "                    recall_val,\n",
    "                    f1_score_val,\n",
    "                )\n",
    "\n",
    "        avg_accuracy = total_accuracy / len(self.confusion_mat)\n",
    "        avg_precision = total_precision / len(self.confusion_mat)\n",
    "        avg_recall = total_recall / len(self.confusion_mat)\n",
    "        avg_f1_score = total_f1_score / len(self.confusion_mat)\n",
    "\n",
    "        if(self.log_level==\"macro\"):\n",
    "            self.print_eval(\"Macro Average:\", avg_accuracy, avg_precision, avg_recall, avg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f02916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictFNN(csvFile: str):\n",
    "    df = pd.read_csv(csvFile)\n",
    "    texts = df[\"text\"].astype(str).apply(cleanerFunction).tolist()\n",
    "    \n",
    "    ckpt = torch.load(\"savedModel/ffnn.pth\", map_location = \"cpu\")\n",
    "    sd = ckpt[\"model_state_dict\"]\n",
    "    w2i = ckpt[\"wordtoindex\"]\n",
    "    padIndex = ckpt[\"padIndex\"]\n",
    "    maxseqlen = ckpt[\"max_sequence_length\"]\n",
    "\n",
    "    vocabSize, embeddingDim = sd[\"embedding.weight\"].shape\n",
    "    hiddenUnit1 = sd[\"fc1.weight\"].shape[0]\n",
    "    hiddenUnit2 = sd[\"fc2.weight\"].shape[0]\n",
    "    \n",
    "    model = feedforwardNeuralNetwork(\n",
    "        vocabSize = vocabSize,\n",
    "        embeddingDim = embeddingDim,\n",
    "        hiddenUnit1 = hiddenUnit1,\n",
    "        hiddenUnit2 = hiddenUnit2,\n",
    "        dropoutRate = 0.0,\n",
    "        numLabels = numLabels,\n",
    "        padIndex = padIndex\n",
    "    )\n",
    "    model.load_state_dict(sd)\n",
    "    model.eval()\n",
    "\n",
    "    x = encodePadFunction(texts, w2i, maxseqlen)\n",
    "    xTensor = torch.from_numpy(x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(xTensor)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).long()\n",
    "    return preds.tolist()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c9fa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLSTM(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    texts = df[\"text\"].astype(str).apply(cleanerFunction).tolist()\n",
    "\n",
    "    ckpt = torch.load(\"savedModel/rnnLSTM.pth\", map_location = \"cpu\", weights_only = False)\n",
    "    sd = ckpt[\"model_state_dict\"]\n",
    "    w2i = ckpt[\"wordtoindex\"]\n",
    "    pad = ckpt[\"padIndex\"]\n",
    "    maxseqlen = ckpt[\"max_sequence_length\"]\n",
    "    hp = ckpt[\"hyperparameters\"]\n",
    "\n",
    "    vocabSize, embeddingDim = sd[\"embedding.weight\"].shape\n",
    "    numLabels = sd[\"output_layer.weight\"].shape[0]\n",
    "    denseUnits, inFeat = sd[\"fc1.weight\"].shape\n",
    "    factor = 2 if hp[\"lstm_bidirectional\"] else 1\n",
    "    hiddenDim = inFeat // factor\n",
    "\n",
    "    bidirectional = hp[\"lstm_bidirectional\"]\n",
    "    useAttention = hp[\"lstm_useAttention\"]\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        vocabSize, \n",
    "        embeddingDim,\n",
    "        hiddenDim,\n",
    "        rnnLayers = 1,\n",
    "        bidirectional = bidirectional,\n",
    "        dropoutRate = 0.0,\n",
    "        denseUnits = denseUnits,\n",
    "        numLabels = numLabels,\n",
    "        padIndex = pad,\n",
    "        useAttention = useAttention\n",
    "    )\n",
    "    model.load_state_dict(sd, strict = False)\n",
    "    model.eval()\n",
    "\n",
    "    x = encodePadFunction(texts, w2i, maxseqlen)\n",
    "    xTensor = torch.from_numpy(x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(xTensor)\n",
    "        probs = torch.sigmoid(logits)\n",
    "    \n",
    "    preds = (probs >= 0.5).int()\n",
    "    return preds.tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c15a5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictGRU(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    texts = df[\"text\"].astype(str).apply(cleanerFunction).tolist()\n",
    "\n",
    "    ckpt = torch.load(\"savedModel/rnnGRU.pth\", map_location = \"cpu\", weights_only = False)\n",
    "    sd = ckpt[\"model_state_dict\"]\n",
    "    w2i = ckpt[\"wordtoindex\"]\n",
    "    pad = ckpt[\"padIndex\"]\n",
    "    maxseqlen = ckpt[\"max_sequence_length\"]\n",
    "    hp = ckpt[\"hyperparameters\"]\n",
    "\n",
    "    vocabSize, embeddingDim = sd[\"embedding.weight\"].shape\n",
    "    numLabels = sd[\"output_layer.weight\"].shape[0]\n",
    "    denseUnits, inFeat = sd[\"fc1.weight\"].shape\n",
    "\n",
    "    bidirectional = hp[\"gru_bidirectional\"]\n",
    "    useAttention  = hp[\"gru_attn\"]\n",
    "    rnnLayers = hp[\"gru_layers\"]\n",
    "\n",
    "    factor = 2 if bidirectional else 1\n",
    "    hiddenDim = inFeat // factor\n",
    "\n",
    "    model = GRUClassifier(\n",
    "        vocabSize, \n",
    "        embeddingDim,\n",
    "        hiddenDim,\n",
    "        rnnLayers = rnnLayers,\n",
    "        bidirectional = bidirectional,\n",
    "        dropoutRate = 0.0,\n",
    "        denseUnits = denseUnits,\n",
    "        numLabels = numLabels,\n",
    "        padIndex = pad,\n",
    "        useAttention = useAttention\n",
    "    )\n",
    "    model.load_state_dict(sd)\n",
    "    model.eval()\n",
    "\n",
    "    x = encodePadFunction(texts, w2i, maxseqlen)\n",
    "    xTensor = torch.from_numpy(x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(xTensor)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "    preds = (probs >= 0.5).int()\n",
    "    return preds.tolist()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e79163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(csvPath: str, pred, predname):\n",
    "    df = pd.read_csv(csvPath)\n",
    "    label_cols = [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "    yTrue = df[label_cols].values.astype(int)\n",
    "\n",
    "    yPred = np.array(pred(csvPath), dtype = int)\n",
    "\n",
    "    hammingAccuracy = (yPred == yTrue).mean()\n",
    "\n",
    "    exactMatch = np.all(yPred == yTrue, axis = 1).mean()\n",
    "\n",
    "    labelAccuracy = (yPred == yTrue).mean(axis = 0)\n",
    "\n",
    "    print(f\"Metrics from {predname}\")\n",
    "\n",
    "    print(f\"Hamming Accuracy : {hammingAccuracy:.4f}\")\n",
    "    print(f\"Exact窶信atch Ratio : {exactMatch:.4f}\")\n",
    "    for label, acc in zip(label_cols, labelAccuracy):\n",
    "        print(f\"{label:10s} Accuracy : {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rfc(csvPath: str, log_level: str):\n",
    "    rfcModel = joblib.load(\"savedModel/rfcmodel.joblib\")\n",
    "    df = pd.read_csv(csvPath)\n",
    "    \n",
    "    if(log_level == \"emotions\"): \n",
    "        title = \"Emotion: {labelColumns[i]}\"\n",
    "    elif(log_level == \"macro\"):\n",
    "        title = \"Macro Average:\"\n",
    "    else:\n",
    "        raise Exception(\"Invalid log level\")\n",
    "\n",
    "    x_texts = df['text']\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_texts_vec = vectorizer.fit_transform(x_texts)\n",
    "    \n",
    "    y_actual = df[labelColumns].values.astype(int)\n",
    "\n",
    "    x_train_text, x_test_text, y_train_labels, y_test_labels = train_test_split(x_texts_vec, y_actual, test_size = 0.1)\n",
    "\n",
    "    rfcModel.fit(x_train_text, y_train_labels)\n",
    "    y_pred = rfcModel.predict(x_test_text)\n",
    "\n",
    "    confusion_mat = multilabel_confusion_matrix(y_test_labels, y_pred)\n",
    "\n",
    "    rfc_metric = metrics_rfc(confusion_mat, log_level, title)\n",
    "    rfc_metric.present_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3af17623",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = os.path.join(\"Data\", \"track-a-test-large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ad18bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\Uni Marburg\\ProjectNLP\\.venv\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\Coding\\Uni Marburg\\ProjectNLP\\.venv\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: anger\n",
      "\n",
      "accuracy: 0.8\n",
      "precision: 0.96\n",
      "recall: 0.82\n",
      "f1 Score: 0.89\n",
      "=======\n",
      "\n",
      "Emotion: fear\n",
      "\n",
      "accuracy: 0.81\n",
      "precision: 0.95\n",
      "recall: 0.84\n",
      "f1 Score: 0.9\n",
      "=======\n",
      "\n",
      "Emotion: joy\n",
      "\n",
      "accuracy: 0.65\n",
      "precision: 0.91\n",
      "recall: 0.69\n",
      "f1 Score: 0.79\n",
      "=======\n",
      "\n",
      "Emotion: sadness\n",
      "\n",
      "accuracy: 0.76\n",
      "precision: 0.97\n",
      "recall: 0.78\n",
      "f1 Score: 0.86\n",
      "=======\n",
      "\n",
      "Emotion: surprise\n",
      "\n",
      "accuracy: 0.77\n",
      "precision: 0.95\n",
      "recall: 0.8\n",
      "f1 Score: 0.87\n",
      "=======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_rfc(testfile, \"emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d368235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from FNN\n",
      "Hamming Accuracy : 0.7602\n",
      "Exact窶信atch Ratio : 0.2430\n",
      "anger      Accuracy : 0.7870\n",
      "fear       Accuracy : 0.7900\n",
      "joy        Accuracy : 0.6550\n",
      "sadness    Accuracy : 0.8140\n",
      "surprise   Accuracy : 0.7550\n"
     ]
    }
   ],
   "source": [
    "evaluate(testfile, predictFNN, \"FNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36ab8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from GRU\n",
      "Hamming Accuracy : 0.7934\n",
      "Exact窶信atch Ratio : 0.3160\n",
      "anger      Accuracy : 0.7870\n",
      "fear       Accuracy : 0.7910\n",
      "joy        Accuracy : 0.7880\n",
      "sadness    Accuracy : 0.8140\n",
      "surprise   Accuracy : 0.7870\n"
     ]
    }
   ],
   "source": [
    "evaluate(testfile, predictGRU, \"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbbe1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from LSTM\n",
      "Hamming Accuracy : 0.7934\n",
      "Exact窶信atch Ratio : 0.3160\n",
      "anger      Accuracy : 0.7870\n",
      "fear       Accuracy : 0.7910\n",
      "joy        Accuracy : 0.7880\n",
      "sadness    Accuracy : 0.8140\n",
      "surprise   Accuracy : 0.7870\n"
     ]
    }
   ],
   "source": [
    "evaluate(testfile, predictLSTM, \"LSTM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
