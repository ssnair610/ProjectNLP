{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ff056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb3b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"Data\\\\track-a.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe28511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34521e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpModel = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bc303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerFunction(text):\n",
    "    tempDoc = nlpModel(text)\n",
    "    token = [\n",
    "        tok.lemma_.lower()\n",
    "        for tok in tempDoc\n",
    "        if not tok.is_stop and not tok.is_punct and tok.lemma_ != \"-PRON-\"\n",
    "    ]\n",
    "    return \" \".join(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f22031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame[\"Spacy_text\"] = dataFrame[\"text\"].astype(str).apply(cleanerFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb565f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColumns = [col for col in [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"] if col in dataFrame]\n",
    "y = dataFrame[labelColumns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9a7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(dataFrame[\"Spacy_text\"], y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f17ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.nn.functional import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903283ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "textTrain, textValue, labelTrain, labelValue = train_test_split(dataFrame[\"Spacy_text\"], y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97148e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "textTrain = textTrain.reset_index(drop = True)\n",
    "textValue = textValue.reset_index(drop = True)\n",
    "\n",
    "textValue = textValue.tolist()\n",
    "textTrain = textTrain.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca4b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "251ebef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentDetect(Dataset):\n",
    "    def __init__(self, text, label, token, maxLength = 128):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.token = token\n",
    "        self.maxLength = maxLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        textchanged = self.text.iloc[i]\n",
    "\n",
    "        enc = self.token(\n",
    "            textchanged, max_length = self.maxLength, truncation = True, padding = \"max_length\", return_tensors = \"pt\"\n",
    "        )\n",
    "        chosenItem = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        chosenItem[\"label\"] = torch.FloatTensor(self.label[i])\n",
    "        return chosenItem\n",
    "    \n",
    "trainSD = sentDetect(xTrain, yTrain, tokenizer)\n",
    "testSD = sentDetect(xTest, yTest, tokenizer)\n",
    "trainDL = DataLoader(trainSD, batch_size = 16, shuffle = True)\n",
    "testDL = DataLoader(testSD, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d276d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "userDevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22008d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tensorModel = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    problem_type = \"multi_label_classification\",\n",
    "    num_labels = len(labelColumns)\n",
    ").to(userDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bdf520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = nn.BCEWithLogitsLoss()\n",
    "\n",
    "steps = len(trainDL) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab2389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optPara = AdamW(tensorModel.parameters(), lr = 1e-5, weight_decay = 0.01)\n",
    "# optPara1 = AdamW(tensorModel.parameters(), lr = 3e-5, weight_decay = 0.01)\n",
    "# optPara2 = AdamW(tensorModel.parameters(), lr = 5e-5, weight_decay = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a11b01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup_steps = int(0.1 * steps)\n",
    "# warmup_steps1 = int(0.3 * steps)\n",
    "# warmup_steps2 = int(0.5 * steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efefd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule = get_linear_schedule_with_warmup(optPara, warmup_steps, steps)\n",
    "# schedule1 = get_linear_schedule_with_warmup(optPara1, warmup_steps1, steps)\n",
    "# schedule2 = get_linear_schedule_with_warmup(optPara2, warmup_steps2, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5a8f2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.clip_grad_norm_(tensorModel.parameters(), max_norm = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ae542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def epochTrain(oPara, selectSch):\n",
    "#     tensorModel.train()\n",
    "#     total = 0\n",
    "#     for batch in trainDL:\n",
    "#         oPara.zero_grad()\n",
    "#         id = batch[\"input_ids\"].to(userDevice)\n",
    "#         mask = batch[\"attention_mask\"].to(userDevice)\n",
    "#         labs = batch[\"label\"].to(userDevice)\n",
    "#         outs = tensorModel(id, attention_mask = mask).logits\n",
    "#         loss = lossFunction(outs, labs)\n",
    "#         loss.backward()\n",
    "#         oPara.step()\n",
    "#         selectSch.step()\n",
    "#         total += loss.item()\n",
    "    \n",
    "#     return total / len(trainDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85e7a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def epochEvaluate():\n",
    "#     tensorModel.eval()\n",
    "#     total = 0\n",
    "#     logits = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in testDL:\n",
    "#             id = batch[\"input_ids\"].to(userDevice)\n",
    "#             mask = batch[\"attention_mask\"].to(userDevice)\n",
    "#             labs = batch[\"label\"].to(userDevice)\n",
    "#             outs = tensorModel(id, attention_mask = mask).logits\n",
    "#             total += lossFunction(outs, labs).item()\n",
    "#             logits.append(outs.cpu().numpy())\n",
    "#     return total / len(testDL), numpy.vstack(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "291bc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochTrain(optimizer, scheduler, use_amp = False):\n",
    "    tensorModel.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in trainDL:\n",
    "        tensorModel.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in trainDL:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(userDevice)\n",
    "            attention_mask = batch[\"attention_mask\"].to(userDevice)\n",
    "            labels = batch[\"label\"].to(userDevice)\n",
    "\n",
    "            with autocast(userDevice.type):\n",
    "                logits = tensorModel(input_ids, attention_mask=attention_mask).logits\n",
    "                loss   = lossFunction(logits, labels)\n",
    "\n",
    "            GradScaler.scale(loss).backward()\n",
    "            GradScaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(tensorModel.parameters(), max_norm = 1.0)\n",
    "\n",
    "            GradScaler.step(optimizer)\n",
    "            GradScaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(trainDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f979090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochEvaluate():\n",
    "    tensorModel.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in testDL:\n",
    "            input_ids = batch[\"input_ids\"].to(userDevice)\n",
    "            attention_mask = batch[\"attention_mask\"].to(userDevice)\n",
    "            labels = batch[\"label\"].to(userDevice)\n",
    "\n",
    "            outputs = tensorModel(input_ids, attention_mask = attention_mask)\n",
    "            logits  = outputs.logits\n",
    "\n",
    "            total_loss += lossFunction(logits, labels).item()\n",
    "\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(testDL)\n",
    "    all_logits = numpy.vstack(all_logits)\n",
    "\n",
    "    return avg_loss, all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0087fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Combination 1:\\n\")\n",
    "# for e in range(5):\n",
    "#     start_time = time.time()\n",
    "#     tl = epochTrain(optPara, schedule)\n",
    "#     vl, lg = epochEvaluate()\n",
    "#     end_time = time.time() - start_time\n",
    "#     print(f\"Epoch: {e + 1}: train_loss = {tl : .4f}, value_loss = {vl : .4f}, time_taken = {end_time : .4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "607b4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Combination 2:\\n\")\n",
    "# for e in range(5):\n",
    "#     start_time = time.time()\n",
    "#     tl = epochTrain(optPara1, schedule1)\n",
    "#     vl, lg = epochEvaluate()\n",
    "#     end_time = time.time() - start_time\n",
    "#     print(f\"Epoch: {e + 1}: train_loss = {tl : .4f}, value_loss = {vl : .4f}, time_taken = {end_time : .4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bbaf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Combination 3:\\n\")\n",
    "# for e in range(5):\n",
    "#     start_time = time.time()\n",
    "#     tl = epochTrain(optPara2, schedule2)\n",
    "#     vl, lg = epochEvaluate()\n",
    "#     end_time = time.time() - start_time\n",
    "#     print(f\"Epoch: {e + 1}: train_loss = {tl : .4f}, value_loss = {vl : .4f}, time_taken = {end_time : .4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
